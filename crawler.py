import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, time
from zoneinfo import ZoneInfo
import time as t
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== API ì¸ì¦ ì •ë³´ ====================
client_id = "R7Q2OeVNhj8wZtNNFBwL"
client_secret = "49E810CBKY"
headers = {"User-Agent": "Mozilla/5.0"}

# ==================== í‚¤ì›Œë“œ ê·¸ë£¹ ====================
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': ['ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
               'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
               'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'],
    'ë§ˆí¬ì¤‘ë¶€': ['ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€',
                 'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€',
                 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'],
    'ì˜ë“±í¬ê´€ì•…': ['ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
                   'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'],
    'ê°•ë‚¨ê´‘ì§„': ['ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
                 'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€',
                 'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€']
}

# ==================== í•˜ì´ë¼ì´íŒ… í•¨ìˆ˜ ====================
def highlight_keywords(text, keywords):
    if not keywords:
        return text
    for kw in keywords:
        text = text.replace(kw, f"<mark>{kw}</mark>")
    return text

# ==================== ê¸°ì‚¬ ë³¸ë¬¸ ë³‘ë ¬ í¬ë¡¤ë§ ====================
def fetch_contents(articles, content_func):
    results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_article = {executor.submit(content_func, art['url']): art for art in articles}
        for future in as_completed(future_to_article):
            art = future_to_article[future]
            try:
                content = future.result()
                art['content'] = content
                results.append(art)
            except:
                continue
    return results

# ==================== ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ ====================
def get_newsis_content(url):
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    content = soup.find("div", class_="viewer")
    return content.get_text(separator="\n", strip=True) if content else None

def get_yonhap_content(url):
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    content = soup.find("div", class_="story-news article")
    return content.get_text(separator="\n", strip=True) if content else None

def get_naver_content(url):
    if "n.news.naver.com" not in url:
        return None
    html = requests.get(url, headers=headers)
    if html.status_code == 200:
        soup = BeautifulSoup(html.text, "html.parser")
        content_div = soup.find("div", id="newsct_article")
        return content_div.get_text(separator="\n", strip=True) if content_div else None
    return None

# ==================== ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜ ====================
def collect_newsis_articles(start_dt, end_dt, status):
    articles, page = [], 1
    while True:
        status.markdown(f"ğŸ”„ ë‰´ì‹œìŠ¤ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
        url = f"https://www.newsis.com/realnews/?cid=realnews&day=today&page={page}"
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.articleList2 > li")
        if not items:
            break
        for item in items:
            title_tag = item.select_one("p.tit > a")
            time_tag = item.select_one("p.time")
            if not (title_tag and time_tag):
                continue
            match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}:\d{2}", time_tag.text)
            if not match:
                continue
            dt = datetime.strptime(match.group(), "%Y.%m.%d %H:%M:%S").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt:
                return fetch_contents(articles, get_newsis_content)
            if dt > end_dt:
                continue
            articles.append({"source": "ë‰´ì‹œìŠ¤", "title": title_tag.text.strip(), "datetime": dt, "url": "https://www.newsis.com" + title_tag['href']})
        page += 1
        t.sleep(0.2)
    return fetch_contents(articles, get_newsis_content)

def collect_yonhap_articles(start_dt, end_dt, status):
    articles, page = [], 1
    while True:
        status.markdown(f"ğŸ”„ ì—°í•©ë‰´ìŠ¤ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
        url = f"https://www.yna.co.kr/news/{page}?site=navi_latest_depth01"
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("ul.list01 > li[data-cid]")
        if not items:
            break
        for item in items:
            title_tag = item.select_one(".tit-wrap .tit-news .title01")
            time_tag = item.select_one(".txt-time")
            if not (title_tag and time_tag):
                continue
            dt = datetime.strptime(f"2025-{time_tag.text.strip()}", "%Y-%m-%d %H:%M").replace(tzinfo=ZoneInfo("Asia/Seoul"))
            if dt < start_dt:
                return fetch_contents(articles, get_yonhap_content)
            if dt > end_dt:
                continue
            articles.append({"source": "ì—°í•©ë‰´ìŠ¤", "title": title_tag.text.strip(), "datetime": dt, "url": f"https://www.yna.co.kr/view/{item['data-cid']}"})
        page += 1
        t.sleep(0.2)
    return fetch_contents(articles, get_yonhap_content)

def collect_naver_articles(start_dt, end_dt, selected_keywords, use_filter, status):
    results, seen = [], set()
    headers_naver = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    for start in range(1, 301, 100):
        status.markdown(f"ğŸ”„ [ë‹¨ë…] ê¸°ì‚¬ {start}ë²ˆì§¸ë¶€í„° ìˆ˜ì§‘ ì¤‘...")
        params = {"query": "[ë‹¨ë…]", "sort": "date", "display": 100, "start": start}
        res = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers_naver, params=params)
        for item in res.json().get("items", []):
            title = BeautifulSoup(item["title"], "html.parser").get_text()
            link = item["link"]
            pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z").astimezone(ZoneInfo("Asia/Seoul"))
            if not (start_dt <= pub_date <= end_dt) or link in seen:
                continue
            body = get_naver_content(link)
            if not body:
                continue
            seen.add(link)
            matched = [kw for kw in selected_keywords if kw in body] if use_filter else selected_keywords
            if use_filter and not matched:
                continue
            results.append({"source": "ë‹¨ë…", "title": title, "datetime": pub_date, "content": body, "matched": matched})
    return results

# ==================== UI ì‹¤í–‰ ====================
st.title("ğŸ“° ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ê¸° (ì—°í•©ë‰´ìŠ¤ Â· ë‰´ì‹œìŠ¤ Â· [ë‹¨ë…])")
now = datetime.now(ZoneInfo("Asia/Seoul"))
today = now.date()
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=today)
    start_time = st.time_input("ì‹œì‘ ì‹œê°", value=time(0, 0))
    start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))
with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=today)
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°", value=time(now.hour, now.minute))
    end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

selected_groups = st.multiselect("ğŸ“š í‚¤ì›Œë“œ ê·¸ë£¹ ì„ íƒ", list(keyword_groups), default=['ì‹œê²½', 'ì¢…í˜œë¶'])
selected_keywords = [kw for group in selected_groups for kw in keyword_groups[group]]
use_filter = st.checkbox("ğŸ” í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ë§Œ ë³´ê¸°", value=True)
collect_general = st.checkbox("ğŸ“° ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘", value=True)
collect_danok = st.checkbox("ğŸ“Œ [ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘", value=True)

if st.button("âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
    status = st.empty()
    general_articles = danok_articles = []
    if collect_general:
        with st.spinner("ì—°í•©ë‰´ìŠ¤/ë‰´ì‹œìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            newsis = collect_newsis_articles(start_dt, end_dt, status)
            yonhap = collect_yonhap_articles(start_dt, end_dt, status)
            general_articles = newsis + yonhap
    if collect_danok:
        with st.spinner("[ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘..."):
            danok_articles = collect_naver_articles(start_dt, end_dt, selected_keywords, use_filter, status)

    if general_articles:
        st.markdown("## â—† ì—°í•©ë‰´ìŠ¤Â·ë‰´ì‹œìŠ¤")
        for a in general_articles:
            st.markdown(f"â–³{a['source']}/{a['title']}")
            st.caption(a['datetime'].strftime('%Y-%m-%d %H:%M'))
            highlighted = highlight_keywords(a['content'], selected_keywords)
            st.markdown(f"- {highlighted}", unsafe_allow_html=True)

    if danok_articles:
        st.markdown("## â—† ë‹¨ë…")
        for a in danok_articles:
            st.markdown(f"â–³ë‹¨ë…/{a['title']}")
            st.caption(a['datetime'].strftime('%Y-%m-%d %H:%M'))
            highlighted = highlight_keywords(a['content'], selected_keywords)
            st.markdown(f"- {highlighted}", unsafe_allow_html=True)

    text_block = "<ë³´ê³ >\n"
    if general_articles:
        text_block += "ã€ì‚¬íšŒë©´ã€‘\n"
        for a in general_articles:
            text_block += f"â–³{a['title']}\n-{a['content']}\n\n"
    if danok_articles:
        text_block += "ã€íƒ€ì§€ã€‘\n"
        for a in danok_articles:
            text_block += f"â–³{a['title']}\n-{a['content']}\n\n"
    st.code(text_block.strip(), language="markdown")
    st.caption("ë³µì‚¬í•´ì„œ ë³´ê³ ì„œ ë“±ì— í™œìš©í•˜ì„¸ìš”.")
