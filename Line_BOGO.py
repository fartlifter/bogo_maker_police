import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, time
from zoneinfo import ZoneInfo
import time as t
from collections import defaultdict
import pandas as pd
import io

# === ì¸ì¦ ì •ë³´ ===
client_id = "R7Q2OeVNhj8wZtNNFBwL"
client_secret = "49E810CBKY"

def parse_pubdate(pubdate_str):
    try:
        return datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def extract_article_text(url):
    try:
        html = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        if html.status_code == 200:
            soup = BeautifulSoup(html.text, "html.parser")
            content_div = soup.find("div", id="newsct_article")
            return content_div.get_text(separator="\n", strip=True) if content_div else None
    except:
        pass
    return None

def extract_media_name(url):
    try:
        domain = url.split("//")[-1].split("/")[0]
        parts = domain.split(".")
        media_key = (
            parts[2] if parts[0] == "www" and parts[1] == "news"
            else parts[1] if parts[0] in ("www", "news")
            else parts[0]
        )
        media_mapping = {
            "chosun": "ì¡°ì„ ", "joongang": "ì¤‘ì•™", "donga": "ë™ì•„", "hani": "í•œê²¨ë ˆ",
            "khan": "ê²½í–¥", "hankookilbo": "í•œêµ­", "segye": "ì„¸ê³„", "seoul": "ì„œìš¸",
            "kmib": "êµ­ë¯¼", "munhwa": "ë¬¸í™”", "kbs": "KBS", "sbs": "SBS",
            "imnews": "MBC", "jtbc": "JTBC", "ichannela": "ì±„ë„A", "tvchosun": "TVì¡°ì„ ",
            "mk": "ë§¤ê²½", "sedaily": "ì„œê²½", "hankyung": "í•œê²½", "news1": "ë‰´ìŠ¤1",
            "newsis": "ë‰´ì‹œìŠ¤", "yna": "ì—°í•©"
        }
        return media_mapping.get(media_key.lower(), media_key.upper())
    except:
        return "[ë§¤ì²´ì¶”ì¶œì‹¤íŒ¨]"

def safe_api_request(url, headers, params, max_retries=3):
    for _ in range(max_retries):
        res = requests.get(url, headers=headers, params=params)
        if res.status_code == 200:
            return res
        t.sleep(0.5)
    return res

# === UI ===
st.title("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°")
st.markdown("âœ… `[ë‹¨ë…] ê¸°ì‚¬`ì™€ `í‚¤ì›Œë“œ ê¸°ì‚¬` ì¤‘ ì›í•˜ëŠ” í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš” (KST ê¸°ì¤€)")

collect_dandok = st.checkbox("ğŸ“Œ [ë‹¨ë…] ê¸°ì‚¬ ìˆ˜ì§‘", value=True)
collect_keywords = st.checkbox("ğŸ“Œ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ ìˆ˜ì§‘", value=True)
if not collect_dandok and not collect_keywords:
    st.warning("í•˜ë‚˜ ì´ìƒì˜ ìˆ˜ì§‘ í•­ëª©ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

# ë‚ ì§œ ë° ì‹œê°„ ì„¤ì •
now = datetime.now(ZoneInfo("Asia/Seoul"))
col1, col2 = st.columns(2)
with col1:
    start_dt = st.datetime_input(
        "ì‹œì‘ ì‹œê°", 
        value=datetime.combine(now.date(), time(0, 0)).replace(tzinfo=ZoneInfo("Asia/Seoul"))
    )
with col2:
    end_dt = st.datetime_input(
        "ì¢…ë£Œ ì‹œê°", 
        value=now
    )

# === í‚¤ì›Œë“œ ëª©ë¡ ë° ê¸°ë³¸ ì„ íƒ ===
all_keywords = [
    'ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘', 'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰',
    'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€',
    'ì„œìš¸ëŒ€ë³‘ì›', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'ì„œìš¸ê²½ì°°ì²­', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ',
    'ê²½ì°°ì²­', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›',
    'ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
    'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€', 'ê±´êµ­ëŒ€',
    'ì„¸ì¢…ëŒ€', 'ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€',
    'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•',
    'êµ°ì¸ê¶Œì„¼í„°', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›', 'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°',
    'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'
]

default_selection = [
    'ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘', 'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰',
    'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€',
    'ì„œìš¸ëŒ€ë³‘ì›', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'ì„œìš¸ê²½ì°°ì²­', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'
]

selected_keywords = []
if collect_keywords:
    selected_keywords = st.multiselect("ğŸ“‚ í‚¤ì›Œë“œ ì„ íƒ", all_keywords, default=default_selection)

# === ì‹¤í–‰ ===
if st.button("âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        status_text = st.empty()
        progress_bar = st.progress(0)
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        seen_links = set()
        grouped = defaultdict(list)
        all_articles = []
        total = 0

        keyword_loop_count = len(selected_keywords) * 10 if collect_keywords else 0
        dandok_loop_count = 10 if collect_dandok else 0
        estimated_loops = keyword_loop_count + dandok_loop_count
        loop_counter = 0

        if collect_dandok:
            st.subheader("ğŸŸ¡ [ë‹¨ë…] ê¸°ì‚¬")
            for start_index in range(1, 1001, 100):
                loop_counter += 1
                progress_bar.progress(min(loop_counter / estimated_loops, 1.0))
                status_text.markdown(f"ğŸŸ¡ [ë‹¨ë…] ìˆ˜ì§‘ ì¤‘... **{total}ê±´ ìˆ˜ì§‘ë¨**")
                params = {
                    "query": "[ë‹¨ë…]",
                    "sort": "date",
                    "display": 100,
                    "start": start_index
                }
                res = safe_api_request("https://openapi.naver.com/v1/search/news.json", headers, params)
                if res.status_code != 200:
                    st.warning(f"[ë‹¨ë…] API í˜¸ì¶œ ì‹¤íŒ¨: {res.status_code}")
                    break
                items = res.json().get("items", [])
                if not items:
                    break
                for item in items:
                    title = BeautifulSoup(item["title"], "html.parser").get_text()
                    if "[ë‹¨ë…]" not in title:
                        continue
                    pub_dt = parse_pubdate(item.get("pubDate"))
                    if not pub_dt or not (start_dt <= pub_dt <= end_dt):
                        continue
                    link = item.get("link")
                    if not link or link in seen_links:
                        continue
                    seen_links.add(link)
                    body = extract_article_text(link)
                    if not body:
                        continue
                    media = extract_media_name(item.get("originallink", ""))
                    all_articles.append({
                        "í‚¤ì›Œë“œ": "[ë‹¨ë…]",
                        "ë§¤ì²´": media,
                        "ì œëª©": title,
                        "ë‚ ì§œ": pub_dt.strftime("%Y-%m-%d %H:%M:%S"),
                        "ë³¸ë¬¸": body
                    })
                    st.markdown(f"**â–³{media}/{title}**")
                    st.caption(pub_dt.strftime("%Y-%m-%d %H:%M:%S"))
                    st.write(f"- {body}")
                    total += 1
                    status_text.markdown(f"ğŸŸ¡ [ë‹¨ë…] ìˆ˜ì§‘ ì¤‘... **{total}ê±´ ìˆ˜ì§‘ë¨**")
                    t.sleep(0.5)

        if collect_keywords:
            st.subheader("ğŸ”µ í‚¤ì›Œë“œ ê¸°ì‚¬ (ì—°í•©/ë‰´ì‹œìŠ¤)")
            for keyword in selected_keywords:
                for start_index in range(1, 1001, 100):
                    loop_counter += 1
                    progress_bar.progress(min(loop_counter / estimated_loops, 1.0))
                    status_text.markdown(f"ğŸ”µ `{keyword}` ìˆ˜ì§‘ ì¤‘... **{total}ê±´ ìˆ˜ì§‘ë¨**")
                    params = {
                        "query": f'"{keyword}"',
                        "sort": "date",
                        "display": 100,
                        "start": start_index
                    }
                    res = safe_api_request("https://openapi.naver.com/v1/search/news.json", headers, params)
                    if res.status_code != 200:
                        st.warning(f"[{keyword}] API í˜¸ì¶œ ì‹¤íŒ¨: {res.status_code}")
                        break
                    items = res.json().get("items", [])
                    if not items:
                        break
                    for item in items:
                        pub_dt = parse_pubdate(item.get("pubDate"))
                        if not pub_dt or not (start_dt <= pub_dt <= end_dt):
                            continue
                        media = extract_media_name(item.get("originallink", ""))
                        if media not in ["ì—°í•©", "ë‰´ì‹œìŠ¤"]:
                            continue
                        link = item.get("link")
                        if not link or link in seen_links:
                            continue
                        seen_links.add(link)
                        body = extract_article_text(link)
                        if not body or keyword not in body:
                            continue
                        title = BeautifulSoup(item["title"], "html.parser").get_text()
                        grouped[keyword].append({
                            "title": title,
                            "media": media,
                            "pubdate": pub_dt,
                            "body": body
                        })
                        all_articles.append({
                            "í‚¤ì›Œë“œ": keyword,
                            "ë§¤ì²´": media,
                            "ì œëª©": title,
                            "ë‚ ì§œ": pub_dt.strftime("%Y-%m-%d %H:%M:%S"),
                            "ë³¸ë¬¸": body
                        })
                        total += 1
                        status_text.markdown(f"ğŸ”µ `{keyword}` ìˆ˜ì§‘ ì¤‘... **{total}ê±´ ìˆ˜ì§‘ë¨**")
                        t.sleep(0.5)

        progress_bar.empty()
        status_text.markdown(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ **{total}ê±´**")
        st.success(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total}ê±´")

        # === ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ===
        df = pd.DataFrame(all_articles)
        if not df.empty:
            excel_buffer = io.BytesIO()
            df.to_excel(excel_buffer, index=False)
            st.download_button(
                label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=excel_buffer.getvalue(),
                file_name="ë‰´ìŠ¤_ìˆ˜ì§‘_ê²°ê³¼.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

            # === í´ë¦½ë³´ë“œ ë³µì‚¬ìš© í…ìŠ¤íŠ¸ ===
            text_block = ""
            for row in all_articles:
                text_block += f"â–³{row['ë§¤ì²´']}/{row['ì œëª©']}\n{row['ë‚ ì§œ']}\n-{row['ë³¸ë¬¸']}\n\n"

            st.text_area("ğŸ“‹ ë³µì‚¬ìš© ì „ì²´ ê¸°ì‚¬", text_block.strip(), height=300)
