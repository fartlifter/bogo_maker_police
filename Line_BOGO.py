import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, time
import time as t

# === API ì¸ì¦ ì •ë³´ ===
client_id = "R7Q2OeVNhj8wZtNNFBwL"
client_secret = "49E810CBKY"

# === ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ ===
def parse_pubdate(pubdate_str):
    try:
        return datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

# === ë³¸ë¬¸ ì¶”ì¶œ ===
def extract_article_text(url):
    try:
        html = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        if html.status_code == 200:
            soup = BeautifulSoup(html.text, "html.parser")
            content_div = soup.find("div", id="newsct_article")
            return content_div.get_text(separator="\n", strip=True) if content_div else "[ë³¸ë¬¸ ì—†ìŒ]"
        return f"[ìš”ì²­ ì‹¤íŒ¨: {html.status_code}]"
    except Exception as e:
        return f"[ì˜ˆì™¸ ë°œìƒ: {e}]"

# === ë§¤ì²´ëª… ì¶”ì¶œ ===
def extract_media_name(url):
    try:
        domain = url.split("//")[-1].split("/")[0]
        parts = domain.split(".")
        if parts[0] == "www" and parts[1] == "news":
            media_key = parts[2]
        elif parts[0] == "www":
            media_key = parts[1]
        elif parts[0] == "news":
            media_key = parts[1]
        else:
            media_key = parts[0]

        media_mapping = {
            "chosun": "ì¡°ì„ ", "joongang": "ì¤‘ì•™", "donga": "ë™ì•„", "hani": "í•œê²¨ë ˆ",
            "khan": "ê²½í–¥", "hankookilbo": "í•œêµ­", "segye": "ì„¸ê³„", "seoul": "ì„œìš¸",
            "kmib": "êµ­ë¯¼", "munhwa": "ë¬¸í™”", "kbs": "KBS", "sbs": "SBS",
            "imnews": "MBC", "jtbc": "JTBC", "ichannela": "ì±„ë„A", "tvchosun": "TVì¡°ì„ ",
            "mk": "ë§¤ê²½", "sedaily": "ì„œê²½", "hankyung": "í•œê²½", "news1": "ë‰´ìŠ¤1",
            "newsis": "ë‰´ì‹œìŠ¤", "yna": "ì—°í•©"
        }
        return media_mapping.get(media_key.lower(), media_key.upper())
    except:
        return "[ë§¤ì²´ì¶”ì¶œì‹¤íŒ¨]"

# === Streamlit ì•± ì‹œì‘ ===
st.title("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°")
st.markdown("ì§€ì •í•œ ë‚ ì§œ ë° ì‹œê°„ ë²”ìœ„ì˜ `[ë‹¨ë…]` ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë³¸ë¬¸ì„ ì¶œë ¥í•©ë‹ˆë‹¤.")

# ë‚ ì§œ ë° ì‹œê°„ ì…ë ¥
selected_date = st.date_input("ë‚ ì§œ", value=datetime.today())
col1, col2 = st.columns(2)
with col1:
    start_time = st.time_input("ì‹œì‘ ì‹œê°", value=time(0, 0))
with col2:
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°", value=time(23, 59))

# ê²°í•©í•˜ì—¬ datetime ê°ì²´ë¡œ
start_datetime = datetime.combine(selected_date, start_time)
end_datetime = datetime.combine(selected_date, end_time)

# ìˆ˜ì§‘ ì‹œì‘ ë²„íŠ¼
if st.button("âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘"):
    start_index = 1
    keep_collecting = True
    result_count = 0

    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        while keep_collecting:
            url = "https://openapi.naver.com/v1/search/news.json"
            headers = {
                "X-Naver-Client-Id": client_id,
                "X-Naver-Client-Secret": client_secret
            }
            params = {
                "query": "[ë‹¨ë…]",
                "sort": "date",
                "display": 100,
                "start": start_index
            }

            res = requests.get(url, headers=headers, params=params)
            if res.status_code != 200:
                st.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {res.status_code}")
                break

            items = res.json().get("items", [])
            if not items:
                break

            for item in items:
                title = BeautifulSoup(item["title"], "html.parser").get_text()
                if "[ë‹¨ë…]" not in title:
                    continue

                pub_date_str = item["pubDate"]
                pub_date_dt = parse_pubdate(pub_date_str)
                if not pub_date_dt:
                    continue

                if pub_date_dt < start_datetime:
                    keep_collecting = False
                    break

                if pub_date_dt >= end_datetime:
                    continue

                media = extract_media_name(item.get("originallink", ""))
                link = item["link"]
                body = extract_article_text(link)
                result_count += 1

                st.markdown(f"### â–³{media}/{title}")
                st.caption(pub_date_str)
                st.write(f"- {body}")

                t.sleep(0.5)

            start_index += 100

    if result_count == 0:
        st.info("ì§€ì •í•œ ì‹œê°„ ë²”ìœ„ì—ì„œ [ë‹¨ë…] ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.success(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {result_count}ê±´ ê¸°ì‚¬ ì¶œë ¥ë¨.")
